# gpt20_model.py

# GPT-OSS-20B + LoRA/QLoRA ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© (A100 ìµœì í™”)ë¡œë”© ìœ í‹¸ë¦¬í‹°

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch
import gc

# A100 í˜¸í™˜ì„± ì²´í¬
def check_gpu_compatibility():
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available")
    
    device_name = torch.cuda.get_device_name()
    print(f"GPU: {device_name}")
    print(f"CUDA version: {torch.version.cuda}")
    print(f"PyTorch version: {torch.__version__}")
    
    # A100 í™•ì¸
    if "A100" in device_name:
        print("âœ… A100 detected - enabling optimizations")
        return True
    else:
        print(f"âš ï¸  Not A100, but proceeding with {device_name}")
        return False



def load_tokenizer(model_name_or_path: str):
    print(f"Loading tokenizer: {model_name_or_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, 
            use_fast=True,
        )
    except Exception as e:
        print(f"Fast tokenizer failed: {e}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, 
            use_fast=False,
        )
     
    tokenizer.pad_token = tokenizer.eos_token
    return tokenizer



def load_lora_model(
    model_name_or_path: str, 
    lora_r=8, 
    lora_alpha=16, 
    lora_dropout=0.05, 
    use_qlora=False,
    use_mxfp4=False,
    device_map="auto"
):
    is_a100 = check_gpu_compatibility()
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()

    print(f"Loading model: {model_name_or_path}")
    print(f"Model may already be quantized, loading without additional quantization...")


    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        device_map=device_map,
        dtype=torch.bfloat16 if is_a100 else torch.float16,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
    )
    
    # ì–‘ìží™” ìƒíƒœ í™•ì¸
    if hasattr(model.config, 'quantization_config') and hasattr(model.config.quantization_config, 'quant_method'):
        print(f"âœ… Model loaded with {model.config.quantization_config.quant_method} quantization")
    else:
        print("âœ… Model loaded")

    


    # A100ì— ìµœì í™”ëœ LoRA ì„¤ì •
    if is_a100:
        # A100ì€ ë” í° LoRA rank ì²˜ë¦¬ ê°€ëŠ¥
        lora_r = min(lora_r * 2, 32)  # rankë¥¼ 2ë°°ë¡œ, ìµœëŒ€ 32
        lora_alpha = lora_r * 2  # alphaë„ ë¹„ë¡€ì ìœ¼ë¡œ ì¦ê°€
    
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none", 
        task_type="CAUSAL_LM",
        # A100ì—ì„œëŠ” ë” ë§Žì€ ëª¨ë“ˆ íƒ€ê²Ÿ ê°€ëŠ¥
        target_modules=[
            "q_proj", "v_proj", "k_proj", "o_proj",  # attention
            "gate_proj", "up_proj", "down_proj"      # MLP (if available)
        ]
    )
    
    print(f"Applying LoRA (r={lora_r}, alpha={lora_alpha})...")
    

    try:
        model = get_peft_model(model, lora_config)
        print("âœ… LoRA applied successfully")
    except Exception as e:
        print(f"Error applying LoRA: {e}")
        # íƒ€ê²Ÿ ëª¨ë“ˆì´ ë§žì§€ ì•Šì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìž¬ì‹œë„
        print("Retrying with basic target modules...")
        lora_config.target_modules = ["q_proj", "v_proj"]  # ê¸°ë³¸ì ì¸ attention ëª¨ë“ˆë§Œ
        model = get_peft_model(model, lora_config)
        print("âœ… LoRA applied with basic target modules")
    

    # A100 ìµœì í™” ì„¤ì •
    if is_a100:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print("âœ… TensorFloat-32 enabled for A100")

    # LoRA íŒŒë¼ë¯¸í„° gradient í™œì„±í™” ê°•ì œ í™•ì¸
    print("ðŸ”§ Ensuring LoRA parameters are trainable...")
    lora_param_count = 0
    for name, param in model.named_parameters():
        if 'lora' in name.lower():
            if not param.requires_grad:
                param.requires_grad = True
                print(f"  ðŸ”§ Fixed gradient for: {name}")
            lora_param_count += 1
            
    model.train()
    
    # ìµœì¢… gradient ìƒíƒœ í™•ì¸
    trainable_count = sum(1 for p in model.parameters() if p.requires_grad)
    total_count = sum(1 for p in model.parameters())
    
    print(f"âœ… LoRA parameters found: {lora_param_count}")
    print(f"âœ… Trainable parameters: {trainable_count:,}/{total_count:,}")
    
    if trainable_count == 0:
        raise RuntimeError("âŒ CRITICAL: No trainable parameters found! LoRA may have failed.")
    
    
    # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    
    try:
        # PEFT ëª¨ë¸ì˜ ê²½ìš° get_nb_trainable_parameters()ê°€ íŠœí”Œì„ ë°˜í™˜í•  ìˆ˜ ìžˆìŒ
        trainable_info = model.get_nb_trainable_parameters()
        if isinstance(trainable_info, tuple):
            trainable_params = trainable_info[0] if len(trainable_info) > 0 else 0
        else:
            trainable_params = trainable_info
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Total parameters: {total_params:,}")
        if total_params > 0:
            print(f"Trainable %: {100 * trainable_params / total_params:.2f}%")
    except Exception as e:
        print(f"Could not calculate parameter statistics: {e}")
        # ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Trainable parameters (manual): {trainable_params:,}")
        print(f"Total parameters (manual): {total_params:,}")
        if total_params > 0:
            print(f"Trainable % (manual): {100 * trainable_params / total_params:.2f}%")
    
    return model


