{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36756a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One block of cross-attention:\n",
    "      - text queries {audio+vision}\n",
    "      - audio queries {text+vision}\n",
    "      - vision queries {text+audio}\n",
    "    Then residual + FFN for each branch.\n",
    "    Shapes (batch_first=True): (B, L, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, nhead=8, ffn_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha_t = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.mha_a = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.mha_v = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.ln_t1 = nn.LayerNorm(d_model); self.ln_a1 = nn.LayerNorm(d_model); self.ln_v1 = nn.LayerNorm(d_model)\n",
    "        self.ln_t2 = nn.LayerNorm(d_model); self.ln_a2 = nn.LayerNorm(d_model); self.ln_v2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        hidden = d_model * ffn_mult\n",
    "        self.ffn_t = nn.Sequential(nn.Linear(d_model, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, d_model))\n",
    "        self.ffn_a = nn.Sequential(nn.Linear(d_model, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, d_model))\n",
    "        self.ffn_v = nn.Sequential(nn.Linear(d_model, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, d_model))\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        \"\"\"\n",
    "        t,a,v : (B, Lt/La/Lv, d_model)\n",
    "        pad_* : (B, L) boolean masks where True = PAD (will be ignored)\n",
    "        \"\"\"\n",
    "        # --- attention masks: need [B, L_kv] -> [B * num_heads, L_q, L_kv] handled internally by PyTorch via key_padding_mask\n",
    "        # Text queries others\n",
    "        kv_t = torch.cat([a, v], dim=1)\n",
    "        kpm_t = None\n",
    "        if (pad_a is not None) or (pad_v is not None):\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2], dtype=torch.bool, device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2], dtype=torch.bool, device=v.device)\n",
    "            kpm_t = torch.cat([pad_a, pad_v], dim=1)\n",
    "        t2, _ = self.mha_t(query=t, key=kv_t, value=kv_t, key_padding_mask=kpm_t)\n",
    "        t = self.ln_t1(t + self.drop(t2))\n",
    "        t = self.ln_t2(t + self.drop(self.ffn_t(t)))\n",
    "\n",
    "        # Audio queries others\n",
    "        kv_a = torch.cat([t, v], dim=1)\n",
    "        kpm_a = None\n",
    "        if (pad_t is not None) or (pad_v is not None):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2], dtype=torch.bool, device=t.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2], dtype=torch.bool, device=v.device)\n",
    "            kpm_a = torch.cat([pad_t, pad_v], dim=1)\n",
    "        a2, _ = self.mha_a(query=a, key=kv_a, value=kv_a, key_padding_mask=kpm_a)\n",
    "        a = self.ln_a1(a + self.drop(a2))\n",
    "        a = self.ln_a2(a + self.drop(self.ffn_a(a)))\n",
    "\n",
    "        # Vision queries others\n",
    "        kv_v = torch.cat([t, a], dim=1)\n",
    "        kpm_v = None\n",
    "        if (pad_t is not None) or (pad_a is not None):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2], dtype=torch.bool, device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2], dtype=torch.bool, device=a.device)\n",
    "            kpm_v = torch.cat([pad_t, pad_a], dim=1)\n",
    "        v2, _ = self.mha_v(query=v, key=kv_v, value=kv_v, key_padding_mask=kpm_v)\n",
    "        v = self.ln_v1(v + self.drop(v2))\n",
    "        v = self.ln_v2(v + self.drop(self.ffn_v(v)))\n",
    "\n",
    "        return t, a, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d810bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriModalCrossAttnClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Project (BERT, HuBERT, CLIP) to a shared space and run N cross-attn blocks.\n",
    "    Accepts variable sequence lengths with padding masks.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_text, d_audio, d_vision, d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1, pool=\"cls_mean\"):\n",
    "        super().__init__()\n",
    "        self.proj_t = nn.Linear(d_text, d_model)\n",
    "        self.proj_a = nn.Linear(d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(d_vision, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([CrossAttnBlock(d_model, nhead, dropout=dropout) for _ in range(depth)])\n",
    "\n",
    "        self.pool = pool  # \"cls_mean\" or \"mean\"\n",
    "        # optional learned CLS tokens for each modality\n",
    "        self.cls_t = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.cls_a = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.cls_v = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model * 3)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def _prepend_cls(self, x, cls, pad=None):\n",
    "        B = x.size(0)\n",
    "        cls_exp = cls.expand(B, -1, -1)  # (B,1,d)\n",
    "        x = torch.cat([cls_exp, x], dim=1)\n",
    "        if pad is not None:\n",
    "            pad = torch.cat([torch.zeros(B,1, dtype=torch.bool, device=x.device), pad], dim=1)\n",
    "        return x, pad\n",
    "\n",
    "    def forward(self, text_emb, audio_emb, vision_emb,\n",
    "                pad_t=None, pad_a=None, pad_v=None):\n",
    "        \"\"\"\n",
    "        text_emb  : (B, Lt, d_text)  BERT token embeddings or last hidden states\n",
    "        audio_emb : (B, La, d_audio) HuBERT frame features\n",
    "        vision_emb: (B, Lv, d_vision) CLIP patch/token embeddings (or sequence of MFCC patches)\n",
    "        pad_*     : (B, L) boolean masks (True for PAD). Optional.\n",
    "        \"\"\"\n",
    "        t = self.proj_t(text_emb); a = self.proj_a(audio_emb); v = self.proj_v(vision_emb)\n",
    "\n",
    "        # add CLS tokens for stable pooling\n",
    "        t, pad_t = self._prepend_cls(t, self.cls_t, pad_t)\n",
    "        a, pad_a = self._prepend_cls(a, self.cls_a, pad_a)\n",
    "        v, pad_v = self._prepend_cls(v, self.cls_v, pad_v)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            t, a, v = blk(t, a, v, pad_t, pad_a, pad_v)\n",
    "\n",
    "        # pool\n",
    "        if self.pool == \"cls_mean\":\n",
    "            ft = t[:, 0]  # CLS\n",
    "            fa = a[:, 0]\n",
    "            fv = v[:, 0]\n",
    "        else:  # mean over real tokens (ignoring PAD)\n",
    "            def masked_mean(x, pad):\n",
    "                if pad is None: return x.mean(dim=1)\n",
    "                w = (~pad).float().unsqueeze(-1)  # (B,L,1)\n",
    "                return (x * w).sum(dim=1) / (w.sum(dim=1).clamp_min(1.0))\n",
    "            ft = masked_mean(t, pad_t); fa = masked_mean(a, pad_a); fv = masked_mean(v, pad_v)\n",
    "\n",
    "        z = torch.cat([ft, fa, fv], dim=-1)\n",
    "        z = self.norm(z)\n",
    "        logits = self.head(z)\n",
    "        return logits, (ft, fa, fv), z  # return fused and per-modality reps too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fb897",
   "metadata": {},
   "source": [
    "I need to write this code for PD ReadText, HC Spontaneous and others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880414f",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314cc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Folder paths ---\n",
    "BERT_DIR   = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Bert embedings/HC_Spontaneous_berts_feats_tokens_only\"\n",
    "HUBERT_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Hubert embeddings/HC_Spontaneous_hubert_features\"\n",
    "CLIP_FEATS = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Clip embeddings/HC_Spontaneous_Spectrogram_CLIP_features.npy\"\n",
    "CLIP_NAMES = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Clip embeddings/HC_Spontaneous_Spectrogram_CLIP_filenames.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543db7e9",
   "metadata": {},
   "source": [
    "# Load CLIP embeddings and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb24b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 CLIP embeddings.\n"
     ]
    }
   ],
   "source": [
    "# CLIP features: one large array (N, D)\n",
    "clip_features = np.load(CLIP_FEATS)  # shape (num_samples, clip_dim)\n",
    "with open(CLIP_NAMES, \"r\") as f:\n",
    "    clip_filenames = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# create a mapping {IDxx: feature_row}\n",
    "def extract_id(path):\n",
    "    base = os.path.basename(path)\n",
    "    return base.split(\"_\")[0]  # e.g., \"ID00\"\n",
    "    \n",
    "clip_dict = {extract_id(fname): clip_features[i] for i, fname in enumerate(clip_filenames)}\n",
    "print(f\"Loaded {len(clip_dict)} CLIP embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117aa5b",
   "metadata": {},
   "source": [
    "# Load the BERT and HuBERT embeddings from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be12f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(filename):\n",
    "    \"\"\"\n",
    "    Extracts the sample ID from filenames like:\n",
    "    'ID00_hc_0_0_0s_tokens_for_selfattn.npz' → 'ID00'\n",
    "    \"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    return base.split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1620bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 samples across all three modalities: ['ID00', 'ID01', 'ID03', 'ID05', 'ID08', 'ID09', 'ID10', 'ID11', 'ID12', 'ID14', 'ID15', 'ID19', 'ID21', 'ID22hc', 'ID23', 'ID25', 'ID26', 'ID28', 'ID31', 'ID35']\n"
     ]
    }
   ],
   "source": [
    "def load_np(path):\n",
    "    try:\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "        return None\n",
    "    if isinstance(data, np.lib.npyio.NpzFile):\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "    return data\n",
    "\n",
    "# Build dicts robustly (accept .npy and .npz, skip non-files, handle load errors).\n",
    "# Reuse get_id defined earlier in the notebook (do not redefine it here).\n",
    "bert_dict = {}\n",
    "for f in os.listdir(BERT_DIR):\n",
    "    p = os.path.join(BERT_DIR, f)\n",
    "    if not os.path.isfile(p):\n",
    "        continue\n",
    "    if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "        continue\n",
    "    arr = load_np(p)\n",
    "    if arr is None:\n",
    "        continue\n",
    "    bert_dict[get_id(f)] = arr\n",
    "\n",
    "hubert_dict = {}\n",
    "for f in os.listdir(HUBERT_DIR):\n",
    "    p = os.path.join(HUBERT_DIR, f)\n",
    "    if not os.path.isfile(p):\n",
    "        continue\n",
    "    if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "        continue\n",
    "    arr = load_np(p)\n",
    "    if arr is None:\n",
    "        continue\n",
    "    hubert_dict[get_id(f)] = arr\n",
    "\n",
    "common_ids = sorted(set(bert_dict) & set(hubert_dict) & set(clip_dict))\n",
    "print(f\"Found {len(common_ids)} samples across all three modalities: {common_ids[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016a1f8",
   "metadata": {},
   "source": [
    "# Convert to tensors and pad for Cross-Attention input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97def971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_len(x, max_len=256):\n",
    "    # Accept numpy arrays or torch tensors, and handle 1D/2D/ND inputs.\n",
    "    # If input has more than 2 dims, collapse leading dims into the time dimension.\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    else:\n",
    "        x = x.float()\n",
    "    if x.dim() == 1:  # 1D (CLIP)\n",
    "        x = x.unsqueeze(0)  # add time dim\n",
    "    elif x.dim() > 2:\n",
    "        # flatten leading dims into a single time dimension: (..., L, D) -> (L_total, D)\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    pad = torch.zeros(max_len - L, D, dtype=x.dtype, device=x.device)\n",
    "    return torch.cat([x, pad], dim=0)\n",
    "\n",
    "text_embs, audio_embs, vision_embs, labels = [], [], [], []\n",
    "\n",
    "for sid in common_ids:\n",
    "    t = bert_dict[sid]\n",
    "    a = hubert_dict[sid]\n",
    "    v = clip_dict[sid]\n",
    "\n",
    "    # convert to tensors (if needed) and normalize along feature dim\n",
    "    t = torch.tensor(t, dtype=torch.float32) if not torch.is_tensor(t) else t.float()\n",
    "    a = torch.tensor(a, dtype=torch.float32) if not torch.is_tensor(a) else a.float()\n",
    "    v = torch.tensor(v, dtype=torch.float32) if not torch.is_tensor(v) else v.float()\n",
    "\n",
    "    t = F.normalize(t, dim=-1)\n",
    "    a = F.normalize(a, dim=-1)\n",
    "    v = F.normalize(v, dim=-1)\n",
    "\n",
    "    # pad (only for sequence-based ones)\n",
    "    t = pad_to_len(t)\n",
    "    a = pad_to_len(a)\n",
    "    v = pad_to_len(v)  # will just add one row if 1D\n",
    "\n",
    "    text_embs.append(t.unsqueeze(0))\n",
    "    audio_embs.append(a.unsqueeze(0))\n",
    "    vision_embs.append(v.unsqueeze(0))\n",
    "    labels.append(0 if \"hc\" in sid.lower() else 1)\n",
    "\n",
    "text_embs  = torch.cat(text_embs, dim=0)\n",
    "audio_embs = torch.cat(audio_embs, dim=0)\n",
    "vision_embs= torch.cat(vision_embs, dim=0)\n",
    "labels = torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ce974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, d_text, d_audio, d_vision, d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj_t = nn.Linear(d_text, d_model)\n",
    "        self.proj_a = nn.Linear(d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(nn.Linear(d_model, d_model*4), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model*4, d_model)),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            }) for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        t = self.proj_t(t); a = self.proj_a(a); v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2], dtype=torch.bool, device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2], dtype=torch.bool, device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2], dtype=torch.bool, device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)  # (B,1,d)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4b74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Shape of fused embedding: torch.Size([21, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Assuming you have the FusionTokenCrossAttn class defined (from earlier)\n",
    "model = FusionTokenCrossAttn(\n",
    "    d_text=text_embs.size(-1),\n",
    "    d_audio=audio_embs.size(-1),\n",
    "    d_vision=vision_embs.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "logits, fused = model(text_embs, audio_embs, vision_embs)\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(\"Predictions:\", preds)\n",
    "print(\"Shape of fused embedding:\", fused.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e44ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c979cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Fused_embeddings/HC_Spontaneous_fused_embeddings.npz and .pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, os\n",
    "\n",
    "# Suppose you already have:\n",
    "# fused: (B, 512)  logits: (B, 2)  labels: (B,)\n",
    "# common_ids: list[str] length B, in the SAME order as the batch\n",
    "\n",
    "save_dir = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Fused_embeddings/\"\n",
    "\n",
    "# ensure target directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save NumPy compressed file (ids as ndarray)\n",
    "np.savez_compressed(\n",
    "    os.path.join(save_dir, \"HC_Spontaneous_fused_embeddings.npz\"),\n",
    "    ids=np.array(common_ids, dtype='<U'),  # store as fixed-length unicode array\n",
    "    fused=fused.detach().cpu().numpy(),\n",
    "    labels=labels.detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "# (optional) also save as torch .pt — store ids as numpy array for consistency\n",
    "torch.save(\n",
    "    {\"ids\": np.array(common_ids, dtype='<U'),\n",
    "     \"fused\": fused.detach().cpu(),\n",
    "     \"labels\": labels.detach().cpu()},\n",
    "    os.path.join(save_dir, \"HC_Spontaneous_fused_embeddings.pt\")\n",
    ")\n",
    "\n",
    "print(\"Saved:\",\n",
    "      os.path.join(save_dir, \"HC_Spontaneous_fused_embeddings.npz\"),\n",
    "      \"and .pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c41f6",
   "metadata": {},
   "source": [
    "# HC ReadText Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae5b39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Folder paths ---\n",
    "HC_BERT_DIR   = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Bert embedings/HC_transcript_berts_feats_tokens_only\"\n",
    "HC_HUBERT_DIR = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Hubert embeddings/HC_ReadText_hubert_features\"\n",
    "HC_CLIP_FEATS = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Clip embeddings/HC_ReadText_Spectrogram_CLIP_features.npy\"\n",
    "HC_CLIP_NAMES = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/Clip embeddings/HC_ReadText_Spectrogram_CLIP_filenames.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54fb33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 CLIP embeddings.\n"
     ]
    }
   ],
   "source": [
    "# CLIP features: one large array (N, D)\n",
    "hc_clip_features = np.load(HC_CLIP_FEATS)  # shape (num_samples, clip_dim)\n",
    "with open(HC_CLIP_NAMES, \"r\") as f:\n",
    "    hc_clip_filenames = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# create a mapping {IDxx: feature_row}\n",
    "def extract_id(path):\n",
    "    hc_base = os.path.basename(path)\n",
    "    return hc_base.split(\"_\")[0]  # e.g., \"ID00\"\n",
    "    \n",
    "hc_clip_dict = {extract_id(fname): hc_clip_features[i] for i, fname in enumerate(hc_clip_filenames)}\n",
    "print(f\"Loaded {len(hc_clip_dict)} CLIP embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fac2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(filename):\n",
    "    \"\"\"\n",
    "    Extracts the sample ID from filenames like:\n",
    "    'ID00_hc_0_0_0s_tokens_for_selfattn.npz' → 'ID00'\n",
    "    \"\"\"\n",
    "    hc_base = os.path.basename(filename)\n",
    "    return hc_base.split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "596ef29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 samples across all three modalities: ['ID00', 'ID01', 'ID03', 'ID05', 'ID08', 'ID09', 'ID10', 'ID11', 'ID12', 'ID14', 'ID15', 'ID19', 'ID21', 'ID22', 'ID23', 'ID25', 'ID26', 'ID28', 'ID31', 'ID35']\n"
     ]
    }
   ],
   "source": [
    "def load_np(path):\n",
    "    try:\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "        return None\n",
    "    if isinstance(data, np.lib.npyio.NpzFile):\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "    return data\n",
    "\n",
    "# Build dicts robustly (accept .npy and .npz, skip non-files, handle load errors).\n",
    "# Reuse get_id defined earlier in the notebook (do not redefine it here).\n",
    "hc_bert_dict = {}\n",
    "for f in os.listdir(HC_BERT_DIR):\n",
    "    p = os.path.join(HC_BERT_DIR, f)\n",
    "    if not os.path.isfile(p):\n",
    "        continue\n",
    "    if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "        continue\n",
    "    arr = load_np(p)\n",
    "    if arr is None:\n",
    "        continue\n",
    "    hc_bert_dict[get_id(f)] = arr\n",
    "\n",
    "hc_hubert_dict = {}\n",
    "for f in os.listdir(HC_HUBERT_DIR):\n",
    "    p = os.path.join(HC_HUBERT_DIR, f)\n",
    "    if not os.path.isfile(p):\n",
    "        continue\n",
    "    if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "        continue\n",
    "    arr = load_np(p)\n",
    "    if arr is None:\n",
    "        continue\n",
    "    hc_hubert_dict[get_id(f)] = arr\n",
    "\n",
    "hc_common_ids = sorted(set(hc_bert_dict) & set(hc_hubert_dict) & set(hc_clip_dict))\n",
    "print(f\"Found {len(hc_common_ids)} samples across all three modalities: {hc_common_ids[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1017ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_len(x, max_len=256):\n",
    "    # Accept numpy arrays or torch tensors, and handle 1D/2D/ND inputs.\n",
    "    # If input has more than 2 dims, collapse leading dims into the time dimension.\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    else:\n",
    "        x = x.float()\n",
    "    if x.dim() == 1:  # 1D (CLIP)\n",
    "        x = x.unsqueeze(0)  # add time dim\n",
    "    elif x.dim() > 2:\n",
    "        # flatten leading dims into a single time dimension: (..., L, D) -> (L_total, D)\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    pad = torch.zeros(max_len - L, D, dtype=x.dtype, device=x.device)\n",
    "    return torch.cat([x, pad], dim=0)\n",
    "\n",
    "hc_text_embs, hc_audio_embs, hc_vision_embs, hc_labels = [], [], [], []\n",
    "\n",
    "# Use the HC-specific common ids (hc_common_ids) rather than common_ids from the other dataset.\n",
    "for sid in hc_common_ids:\n",
    "    t = hc_bert_dict[sid]\n",
    "    a = hc_hubert_dict[sid]\n",
    "    v = hc_clip_dict[sid]\n",
    "\n",
    "    # convert to tensors (if needed) and normalize along feature dim\n",
    "    t = torch.tensor(t, dtype=torch.float32) if not torch.is_tensor(t) else t.float()\n",
    "    a = torch.tensor(a, dtype=torch.float32) if not torch.is_tensor(a) else a.float()\n",
    "    v = torch.tensor(v, dtype=torch.float32) if not torch.is_tensor(v) else v.float()\n",
    "\n",
    "    t = F.normalize(t, dim=-1)\n",
    "    a = F.normalize(a, dim=-1)\n",
    "    v = F.normalize(v, dim=-1)\n",
    "\n",
    "    # pad (only for sequence-based ones)\n",
    "    t = pad_to_len(t)\n",
    "    a = pad_to_len(a)\n",
    "    v = pad_to_len(v)  # will just add one row if 1D\n",
    "\n",
    "    hc_text_embs.append(t.unsqueeze(0))\n",
    "    hc_audio_embs.append(a.unsqueeze(0))\n",
    "    hc_vision_embs.append(v.unsqueeze(0))\n",
    "    hc_labels.append(0 if \"hc\" in sid.lower() else 1)\n",
    "\n",
    "hc_text_embs  = torch.cat(hc_text_embs, dim=0)\n",
    "hc_audio_embs = torch.cat(hc_audio_embs, dim=0)\n",
    "hc_vision_embs= torch.cat(hc_vision_embs, dim=0)\n",
    "hc_labels = torch.tensor(hc_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33213158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, hc_d_text, hc_d_audio, hc_d_vision, d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj_t = nn.Linear(hc_d_text, d_model)\n",
    "        self.proj_a = nn.Linear(hc_d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(hc_d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(nn.Linear(d_model, d_model*4), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model*4, d_model)),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            }) for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        t = self.proj_t(t); a = self.proj_a(a); v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2], dtype=torch.bool, device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2], dtype=torch.bool, device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2], dtype=torch.bool, device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)  # (B,1,d)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59435f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Shape of fused embedding: torch.Size([21, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Assuming you have the FusionTokenCrossAttn class defined (from earlier)\n",
    "model = FusionTokenCrossAttn(\n",
    "    hc_d_text=hc_text_embs.size(-1),\n",
    "    hc_d_audio=hc_audio_embs.size(-1),\n",
    "    hc_d_vision=hc_vision_embs.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "hc_logits, fused = model(hc_text_embs, hc_audio_embs, hc_vision_embs)\n",
    "hc_preds = torch.argmax(hc_logits, dim=-1)\n",
    "\n",
    "print(\"Predictions:\", hc_preds)\n",
    "print(\"Shape of fused embedding:\", fused.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9a18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "hc_loss = F.cross_entropy(hc_logits, hc_labels)\n",
    "hc_loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ebc509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/HC_ReadText_Fused_embeddings/HC_ReadText_fused_embeddings.npz and .pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, os\n",
    "\n",
    "# Suppose you already have:\n",
    "# fused: (B, 512)  logits: (B, 2)  labels: (B,)\n",
    "# common_ids: list[str] length B, in the SAME order as the batch\n",
    "\n",
    "save_dir = \"/mnt/d/Roshidat_Msc_Project/Audio_parkinson/pd&Hc_multi/HC_ReadText_Fused_embeddings/\"\n",
    "\n",
    "# ensure target directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save NumPy compressed file (ids as ndarray)\n",
    "np.savez_compressed(\n",
    "    os.path.join(save_dir, \"HC_ReadText_fused_embeddings.npz\"),\n",
    "    ids=np.array(common_ids, dtype='<U'),  # store as fixed-length unicode array\n",
    "    fused=fused.detach().cpu().numpy(),\n",
    "    hc_labels=hc_labels.detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "# (optional) also save as torch .pt — store ids as numpy array for consistency\n",
    "torch.save(\n",
    "    {\"ids\": np.array(common_ids, dtype='<U'),\n",
    "     \"fused\": fused.detach().cpu(),\n",
    "     \"hc_labels\": hc_labels.detach().cpu()},\n",
    "    os.path.join(save_dir, \"HC_ReadText_fused_embeddings.pt\")\n",
    ")\n",
    "\n",
    "print(\"Saved:\",\n",
    "      os.path.join(save_dir, \"HC_ReadText_fused_embeddings.npz\"),\n",
    "      \"and .pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd197f",
   "metadata": {},
   "source": [
    "# PD Spontaneous fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6dd1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ID_RE = re.compile(r'(ID\\d{2,})', re.IGNORECASE)\n",
    "\n",
    "def extract_id_any(s: str) -> str:\n",
    "    \"\"\"Return ID like 'ID00' from any filename/path string.\"\"\"\n",
    "    base = os.path.basename(s)\n",
    "    m = ID_RE.search(base)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not extract ID from: {s}\")\n",
    "    return m.group(1).upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263dbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np(path: str):\n",
    "    try:\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {path}: {e}\")\n",
    "        return None\n",
    "    if isinstance(data, np.lib.npyio.NpzFile):\n",
    "        # pick the first numeric array\n",
    "        for k in data.files:\n",
    "            arr = data[k]\n",
    "            if isinstance(arr, np.ndarray) and np.issubdtype(arr.dtype, np.number):\n",
    "                return arr\n",
    "        return None\n",
    "    return data  # .npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777abf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] PD Spont common: 15 ids\n"
     ]
    }
   ],
   "source": [
    "def build_dict_from_dir(root: str):\n",
    "    d = {}\n",
    "    for f in os.listdir(root):\n",
    "        p = os.path.join(root, f)\n",
    "        if not os.path.isfile(p): \n",
    "            continue\n",
    "        if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "            continue\n",
    "        arr = load_np(p)\n",
    "        if arr is None: \n",
    "            continue\n",
    "        sid = extract_id_any(f)\n",
    "        d[sid] = arr\n",
    "    return d\n",
    "\n",
    "# --- Directories (your PD Spont example) ---\n",
    "PD_BERT_DIR   = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Bert embedings/PD_Spontaneous_berts_feats_tokens_only\"\n",
    "PD_HUBERT_DIR = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Hubert embeddings/PD_Spontaneous_hubert_features\"\n",
    "PD_CLIP_FEATS = \"//home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/PD_Spontaneous_Spectrogram_CLIP_features.npy\"\n",
    "PD_CLIP_NAMES = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/PD_Spontaneous_Spectrogram_CLIP_filenames.txt\"\n",
    "\n",
    "# CLIP (names + big matrix)\n",
    "clip_feats = np.load(PD_CLIP_FEATS)  # [N, D]\n",
    "with open(PD_CLIP_NAMES, \"r\") as f:\n",
    "    clip_names = [line.strip() for line in f]\n",
    "\n",
    "clip_dict = {extract_id_any(name): clip_feats[i] for i, name in enumerate(clip_names)}\n",
    "bert_dict = build_dict_from_dir(PD_BERT_DIR)\n",
    "hub_dict  = build_dict_from_dir(PD_HUBERT_DIR)\n",
    "\n",
    "common_ids = sorted(set(bert_dict) & set(hub_dict) & set(clip_dict))\n",
    "print(f\"[info] PD Spont common: {len(common_ids)} ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80fd423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 256, 768]) torch.Size([15, 256, 768]) torch.Size([15, 256, 512]) 15 torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "def pad_to_len(x, max_len=256):\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "    if x.dim() == 1:  # e.g., CLIP single vector\n",
    "        x = x.unsqueeze(0)\n",
    "    elif x.dim() > 2:\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    return torch.cat([x, torch.zeros(max_len - L, D)], dim=0)\n",
    "\n",
    "# Build tensors\n",
    "text_list, audio_list, vision_list, ids_kept, labels = [], [], [], [], []\n",
    "for sid in common_ids:\n",
    "    t = F.normalize(torch.as_tensor(bert_dict[sid], dtype=torch.float32), dim=-1)\n",
    "    a = F.normalize(torch.as_tensor(hub_dict[sid],  dtype=torch.float32), dim=-1)\n",
    "    v = F.normalize(torch.as_tensor(clip_dict[sid], dtype=torch.float32), dim=-1)\n",
    "\n",
    "    t = pad_to_len(t)    # [256, d_t]\n",
    "    a = pad_to_len(a)    # [256, d_a]\n",
    "    v = pad_to_len(v)    # [256, d_v]\n",
    "\n",
    "    text_list.append(t.unsqueeze(0))\n",
    "    audio_list.append(a.unsqueeze(0))\n",
    "    vision_list.append(v.unsqueeze(0))\n",
    "    ids_kept.append(sid)\n",
    "\n",
    "    # For PD set → label 1; for HC set → label 0 (don’t key off string 'hc' in sid)\n",
    "    labels.append(1)\n",
    "\n",
    "text = torch.cat(text_list, dim=0)\n",
    "audio = torch.cat(audio_list, dim=0)\n",
    "vision= torch.cat(vision_list, dim=0)\n",
    "labels= torch.tensor(labels, dtype=torch.int64)\n",
    "print(text.shape, audio.shape, vision.shape, len(ids_kept), labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2069a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, pd_d_text, pd_d_audio, pd_d_vision,\n",
    "                 d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # project each modality to same dimension\n",
    "        self.proj_t = nn.Linear(pd_d_text, d_model)\n",
    "        self.proj_a = nn.Linear(pd_d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(pd_d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead,\n",
    "                                             dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*4, d_model)\n",
    "                ),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            })\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model),\n",
    "                                  nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        # project\n",
    "        t = self.proj_t(t)\n",
    "        a = self.proj_a(a)\n",
    "        v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "\n",
    "        # build mask if any padding masks were passed\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42356cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred logits shape: torch.Size([15, 2]), fused shape: torch.Size([15, 512])\n"
     ]
    }
   ],
   "source": [
    "model = FusionTokenCrossAttn(\n",
    "    pd_d_text=text.size(-1),\n",
    "    pd_d_audio=audio.size(-1),\n",
    "    pd_d_vision=vision.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "logits, fused = model(text, audio, vision)\n",
    "print(f\"Pred logits shape: {logits.shape}, fused shape: {fused.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef34a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 512) 15 (15,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "out_path = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Fused_embeddings/PD_Spontaneous_fused_embeddings_FIXED.npz\"  # <-- set yours\n",
    "\n",
    "ids_kept = common_ids                    # the exact order you looped\n",
    "labels   = np.full((len(ids_kept),), 1, dtype=np.int64)   # PD = 1\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    fused    = fused.detach().cpu().numpy(),   # [15, 512]\n",
    "    ids      = np.array(ids_kept, dtype=object),\n",
    "    labels   = labels,\n",
    "    kept_idx = np.arange(len(ids_kept), dtype=np.int64),  # optional but handy\n",
    ")\n",
    "# quick sanity check\n",
    "z = np.load(out_path, allow_pickle=True)\n",
    "print(z[\"fused\"].shape, len(z[\"ids\"]), z[\"labels\"].shape)\n",
    "assert z[\"fused\"].shape[0] == len(z[\"ids\"]) == z[\"labels\"].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92155b",
   "metadata": {},
   "source": [
    "# Fusing PD ReadText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034380f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] PD Spont common: 16 ids\n"
     ]
    }
   ],
   "source": [
    "def build_dict_from_dir(root: str):\n",
    "    d = {}\n",
    "    for f in os.listdir(root):\n",
    "        p = os.path.join(root, f)\n",
    "        if not os.path.isfile(p): \n",
    "            continue\n",
    "        if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "            continue\n",
    "        arr = load_np(p)\n",
    "        if arr is None: \n",
    "            continue\n",
    "        sid = extract_id_any(f)\n",
    "        d[sid] = arr\n",
    "    return d\n",
    "\n",
    "# --- Directories (your PD Spont example) ---\n",
    "PD_BERT_DIR   = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Bert embedings/PD_ReadText_berts_feats_tokens_only\"\n",
    "PD_HUBERT_DIR = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Hubert embeddings/PD_ReadText_hubert_features\"\n",
    "PD_CLIP_FEATS = \"//home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/PD_ReadText_Spectrogram_CLIP_features.npy\"\n",
    "PD_CLIP_NAMES = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/PD_ReadText_Spectrogram_CLIP_filenames.txt\"\n",
    "\n",
    "# CLIP (names + big matrix)\n",
    "clip_feats = np.load(PD_CLIP_FEATS)  # [N, D]\n",
    "with open(PD_CLIP_NAMES, \"r\") as f:\n",
    "    clip_names = [line.strip() for line in f]\n",
    "\n",
    "clip_dict = {extract_id_any(name): clip_feats[i] for i, name in enumerate(clip_names)}\n",
    "bert_dict = build_dict_from_dir(PD_BERT_DIR)\n",
    "hub_dict  = build_dict_from_dir(PD_HUBERT_DIR)\n",
    "\n",
    "common_ids = sorted(set(bert_dict) & set(hub_dict) & set(clip_dict))\n",
    "print(f\"[info] PD Spont common: {len(common_ids)} ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c8ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 768]) torch.Size([16, 256, 768]) torch.Size([16, 256, 512]) 16 torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "def pad_to_len(x, max_len=256):\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "    if x.dim() == 1:  # e.g., CLIP single vector\n",
    "        x = x.unsqueeze(0)\n",
    "    elif x.dim() > 2:\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    return torch.cat([x, torch.zeros(max_len - L, D)], dim=0)\n",
    "\n",
    "# Build tensors\n",
    "text_list, audio_list, vision_list, ids_kept, labels = [], [], [], [], []\n",
    "for sid in common_ids:\n",
    "    t = F.normalize(torch.as_tensor(bert_dict[sid], dtype=torch.float32), dim=-1)\n",
    "    a = F.normalize(torch.as_tensor(hub_dict[sid],  dtype=torch.float32), dim=-1)\n",
    "    v = F.normalize(torch.as_tensor(clip_dict[sid], dtype=torch.float32), dim=-1)\n",
    "\n",
    "    t = pad_to_len(t)    # [256, d_t]\n",
    "    a = pad_to_len(a)    # [256, d_a]\n",
    "    v = pad_to_len(v)    # [256, d_v]\n",
    "\n",
    "    text_list.append(t.unsqueeze(0))\n",
    "    audio_list.append(a.unsqueeze(0))\n",
    "    vision_list.append(v.unsqueeze(0))\n",
    "    ids_kept.append(sid)\n",
    "\n",
    "    # For PD set → label 1; for HC set → label 0 (don’t key off string 'hc' in sid)\n",
    "    labels.append(1)\n",
    "\n",
    "text = torch.cat(text_list, dim=0)\n",
    "audio = torch.cat(audio_list, dim=0)\n",
    "vision= torch.cat(vision_list, dim=0)\n",
    "labels= torch.tensor(labels, dtype=torch.int64)\n",
    "print(text.shape, audio.shape, vision.shape, len(ids_kept), labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce005f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, pd_d_text, pd_d_audio, pd_d_vision,\n",
    "                 d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # project each modality to same dimension\n",
    "        self.proj_t = nn.Linear(pd_d_text, d_model)\n",
    "        self.proj_a = nn.Linear(pd_d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(pd_d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead,\n",
    "                                             dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*4, d_model)\n",
    "                ),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            })\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model),\n",
    "                                  nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        # project\n",
    "        t = self.proj_t(t)\n",
    "        a = self.proj_a(a)\n",
    "        v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "\n",
    "        # build mask if any padding masks were passed\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a9d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred logits shape: torch.Size([16, 2]), fused shape: torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "model = FusionTokenCrossAttn(\n",
    "    pd_d_text=text.size(-1),\n",
    "    pd_d_audio=audio.size(-1),\n",
    "    pd_d_vision=vision.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "logits, fused = model(text, audio, vision)\n",
    "print(f\"Pred logits shape: {logits.shape}, fused shape: {fused.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c95aaa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 512) 16 (16,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "out_path = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Fixed_PD_Fused_embeddings/PD_ReadText_fused_embeddings_FIXED.npz\"\n",
    "\n",
    "ids_kept = common_ids                    # the exact order you looped\n",
    "labels   = np.full((len(ids_kept),), 1, dtype=np.int64)   # PD = 1\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "# prepare arrays for saving (detach and move to cpu first)\n",
    "fused_np = fused.detach().cpu().numpy()\n",
    "ids_arr = np.array(ids_kept, dtype='<U')  # store as unicode array\n",
    "kept_idx = np.arange(len(ids_kept), dtype=np.int64)\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    fused    = fused_np,   # [B, D]\n",
    "    ids      = ids_arr,\n",
    "    labels   = labels,\n",
    "    kept_idx = kept_idx,\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "z = np.load(out_path, allow_pickle=True)\n",
    "print(z[\"fused\"].shape, len(z[\"ids\"]), z[\"labels\"].shape)\n",
    "assert z[\"fused\"].shape[0] == len(z[\"ids\"]) == z[\"labels\"].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bb4ec",
   "metadata": {},
   "source": [
    "# HC Spontaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a14719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] HC Spont common: 21 ids\n"
     ]
    }
   ],
   "source": [
    "def build_dict_from_dir(root: str):\n",
    "    d = {}\n",
    "    for f in os.listdir(root):\n",
    "        p = os.path.join(root, f)\n",
    "        if not os.path.isfile(p): \n",
    "            continue\n",
    "        if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "            continue\n",
    "        arr = load_np(p)\n",
    "        if arr is None: \n",
    "            continue\n",
    "        sid = extract_id_any(f)\n",
    "        d[sid] = arr\n",
    "    return d\n",
    "\n",
    "# --- Directories (your PD Spont example) ---\n",
    "HC_BERT_DIR   = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Bert embedings/HC_Spontaneous_berts_feats_tokens_only\"\n",
    "HC_HUBERT_DIR = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Hubert embeddings/HC_Spontaneous_hubert_features\"\n",
    "HC_CLIP_FEATS = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/HC_Spontaneous_Spectrogram_CLIP_features.npy\"\n",
    "HC_CLIP_NAMES = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/HC_Spontaneous_Spectrogram_CLIP_filenames.txt\"\n",
    "\n",
    "# CLIP (names + big matrix)\n",
    "clip_feats = np.load(HC_CLIP_FEATS)  # [N, D]\n",
    "with open(HC_CLIP_NAMES, \"r\") as f:\n",
    "    clip_names = [line.strip() for line in f]\n",
    "\n",
    "clip_dict = {extract_id_any(name): clip_feats[i] for i, name in enumerate(clip_names)}\n",
    "bert_dict = build_dict_from_dir(HC_BERT_DIR)\n",
    "hub_dict  = build_dict_from_dir(HC_HUBERT_DIR)\n",
    "\n",
    "common_ids = sorted(set(bert_dict) & set(hub_dict) & set(clip_dict))\n",
    "print(f\"[info] HC Spont common: {len(common_ids)} ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56845980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 256, 768]) torch.Size([21, 256, 768]) torch.Size([21, 256, 512]) 21 torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "def pad_to_len(x, max_len=256):\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "    if x.dim() == 1:  # e.g., CLIP single vector\n",
    "        x = x.unsqueeze(0)\n",
    "    elif x.dim() > 2:\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    return torch.cat([x, torch.zeros(max_len - L, D)], dim=0)\n",
    "\n",
    "# Build tensors\n",
    "text_list, audio_list, vision_list, ids_kept, labels = [], [], [], [], []\n",
    "for sid in common_ids:\n",
    "    t = F.normalize(torch.as_tensor(bert_dict[sid], dtype=torch.float32), dim=-1)\n",
    "    a = F.normalize(torch.as_tensor(hub_dict[sid],  dtype=torch.float32), dim=-1)\n",
    "    v = F.normalize(torch.as_tensor(clip_dict[sid], dtype=torch.float32), dim=-1)\n",
    "\n",
    "    t = pad_to_len(t)    # [256, d_t]\n",
    "    a = pad_to_len(a)    # [256, d_a]\n",
    "    v = pad_to_len(v)    # [256, d_v]\n",
    "\n",
    "    text_list.append(t.unsqueeze(0))\n",
    "    audio_list.append(a.unsqueeze(0))\n",
    "    vision_list.append(v.unsqueeze(0))\n",
    "    ids_kept.append(sid)\n",
    "\n",
    "    # For PD set → label 1; for HC set → label 0 (don’t key off string 'hc' in sid)\n",
    "    labels.append(1)\n",
    "\n",
    "text = torch.cat(text_list, dim=0)\n",
    "audio = torch.cat(audio_list, dim=0)\n",
    "vision= torch.cat(vision_list, dim=0)\n",
    "labels= torch.tensor(labels, dtype=torch.int64)\n",
    "print(text.shape, audio.shape, vision.shape, len(ids_kept), labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e775455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, pd_d_text, pd_d_audio, pd_d_vision,\n",
    "                 d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # project each modality to same dimension\n",
    "        self.proj_t = nn.Linear(pd_d_text, d_model)\n",
    "        self.proj_a = nn.Linear(pd_d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(pd_d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead,\n",
    "                                             dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*4, d_model)\n",
    "                ),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            })\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model),\n",
    "                                  nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        # project\n",
    "        t = self.proj_t(t)\n",
    "        a = self.proj_a(a)\n",
    "        v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "\n",
    "        # build mask if any padding masks were passed\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91845c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred logits shape: torch.Size([21, 2]), fused shape: torch.Size([21, 512])\n"
     ]
    }
   ],
   "source": [
    "model = FusionTokenCrossAttn(\n",
    "    pd_d_text=text.size(-1),\n",
    "    pd_d_audio=audio.size(-1),\n",
    "    pd_d_vision=vision.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "logits, fused = model(text, audio, vision)\n",
    "print(f\"Pred logits shape: {logits.shape}, fused shape: {fused.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2e81c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 512) 21 (21,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "out_path = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Fixed_HC_Fused_embeddings/HC_Spontaneous_fused_embeddings_FIXED.npz\"\n",
    "\n",
    "ids_kept = common_ids                    # the exact order you looped\n",
    "labels   = np.full((len(ids_kept),), 1, dtype=np.int64)   # PD = 1\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "# prepare arrays for saving (detach and move to cpu first)\n",
    "fused_np = fused.detach().cpu().numpy()\n",
    "ids_arr = np.array(ids_kept, dtype='<U')  # store as unicode array\n",
    "kept_idx = np.arange(len(ids_kept), dtype=np.int64)\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    fused    = fused_np,   # [B, D]\n",
    "    ids      = ids_arr,\n",
    "    labels   = labels,\n",
    "    kept_idx = kept_idx,\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "z = np.load(out_path, allow_pickle=True)\n",
    "print(z[\"fused\"].shape, len(z[\"ids\"]), z[\"labels\"].shape)\n",
    "assert z[\"fused\"].shape[0] == len(z[\"ids\"]) == z[\"labels\"].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489fe9a",
   "metadata": {},
   "source": [
    "# HC ReadText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31063030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] HC Spont common: 21 ids\n"
     ]
    }
   ],
   "source": [
    "def build_dict_from_dir(root: str):\n",
    "    d = {}\n",
    "    for f in os.listdir(root):\n",
    "        p = os.path.join(root, f)\n",
    "        if not os.path.isfile(p): \n",
    "            continue\n",
    "        if not (f.endswith(\".npy\") or f.endswith(\".npz\")):\n",
    "            continue\n",
    "        arr = load_np(p)\n",
    "        if arr is None: \n",
    "            continue\n",
    "        sid = extract_id_any(f)\n",
    "        d[sid] = arr\n",
    "    return d\n",
    "\n",
    "# --- Directories (your PD Spont example) ---\n",
    "HC_BERT_DIR   = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Bert embedings/HC_transcript_berts_feats_tokens_only\"\n",
    "HC_HUBERT_DIR = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Hubert embeddings/HC_ReadText_hubert_features\"\n",
    "HC_CLIP_FEATS = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/HC_ReadText_Spectrogram_CLIP_features.npy\"\n",
    "HC_CLIP_NAMES = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Clip embeddings/HC_ReadText_Spectrogram_CLIP_filenames.txt\"\n",
    "\n",
    "# CLIP (names + big matrix)\n",
    "clip_feats = np.load(HC_CLIP_FEATS)  # [N, D]\n",
    "with open(HC_CLIP_NAMES, \"r\") as f:\n",
    "    clip_names = [line.strip() for line in f]\n",
    "\n",
    "clip_dict = {extract_id_any(name): clip_feats[i] for i, name in enumerate(clip_names)}\n",
    "bert_dict = build_dict_from_dir(HC_BERT_DIR)\n",
    "hub_dict  = build_dict_from_dir(HC_HUBERT_DIR)\n",
    "\n",
    "common_ids = sorted(set(bert_dict) & set(hub_dict) & set(clip_dict))\n",
    "print(f\"[info] HC Spont common: {len(common_ids)} ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab2b480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 256, 768]) torch.Size([21, 256, 768]) torch.Size([21, 256, 512]) 21 torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "def pad_to_len(x, max_len=256):\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "    if x.dim() == 1:  # e.g., CLIP single vector\n",
    "        x = x.unsqueeze(0)\n",
    "    elif x.dim() > 2:\n",
    "        x = x.reshape(-1, x.size(-1))\n",
    "    L, D = x.shape\n",
    "    if L >= max_len:\n",
    "        return x[:max_len]\n",
    "    return torch.cat([x, torch.zeros(max_len - L, D)], dim=0)\n",
    "\n",
    "# Build tensors\n",
    "text_list, audio_list, vision_list, ids_kept, labels = [], [], [], [], []\n",
    "for sid in common_ids:\n",
    "    t = F.normalize(torch.as_tensor(bert_dict[sid], dtype=torch.float32), dim=-1)\n",
    "    a = F.normalize(torch.as_tensor(hub_dict[sid],  dtype=torch.float32), dim=-1)\n",
    "    v = F.normalize(torch.as_tensor(clip_dict[sid], dtype=torch.float32), dim=-1)\n",
    "\n",
    "    t = pad_to_len(t)    # [256, d_t]\n",
    "    a = pad_to_len(a)    # [256, d_a]\n",
    "    v = pad_to_len(v)    # [256, d_v]\n",
    "\n",
    "    text_list.append(t.unsqueeze(0))\n",
    "    audio_list.append(a.unsqueeze(0))\n",
    "    vision_list.append(v.unsqueeze(0))\n",
    "    ids_kept.append(sid)\n",
    "\n",
    "    # For PD set → label 1; for HC set → label 0 (don’t key off string 'hc' in sid)\n",
    "    labels.append(1)\n",
    "\n",
    "text = torch.cat(text_list, dim=0)\n",
    "audio = torch.cat(audio_list, dim=0)\n",
    "vision= torch.cat(vision_list, dim=0)\n",
    "labels= torch.tensor(labels, dtype=torch.int64)\n",
    "print(text.shape, audio.shape, vision.shape, len(ids_kept), labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46ad227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FusionTokenCrossAttn(nn.Module):\n",
    "    def __init__(self, pd_d_text, pd_d_audio, pd_d_vision,\n",
    "                 d_model=512, nhead=8, depth=2, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # project each modality to same dimension\n",
    "        self.proj_t = nn.Linear(pd_d_text, d_model)\n",
    "        self.proj_a = nn.Linear(pd_d_audio, d_model)\n",
    "        self.proj_v = nn.Linear(pd_d_vision, d_model)\n",
    "        self.fuse_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"mha\": nn.MultiheadAttention(d_model, nhead,\n",
    "                                             dropout=dropout, batch_first=True),\n",
    "                \"ln1\": nn.LayerNorm(d_model),\n",
    "                \"ffn\": nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model*4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_model*4, d_model)\n",
    "                ),\n",
    "                \"ln2\": nn.LayerNorm(d_model),\n",
    "            })\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_model),\n",
    "                                  nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, t, a, v, pad_t=None, pad_a=None, pad_v=None):\n",
    "        # project\n",
    "        t = self.proj_t(t)\n",
    "        a = self.proj_a(a)\n",
    "        v = self.proj_v(v)\n",
    "        kv = torch.cat([t, a, v], dim=1)\n",
    "\n",
    "        # build mask if any padding masks were passed\n",
    "        if any(m is not None for m in (pad_t, pad_a, pad_v)):\n",
    "            pad_t = pad_t if pad_t is not None else torch.zeros(t.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=t.device)\n",
    "            pad_a = pad_a if pad_a is not None else torch.zeros(a.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=a.device)\n",
    "            pad_v = pad_v if pad_v is not None else torch.zeros(v.size()[:2],\n",
    "                                                                dtype=torch.bool,\n",
    "                                                                device=v.device)\n",
    "            kpm = torch.cat([pad_t, pad_a, pad_v], dim=1)\n",
    "        else:\n",
    "            kpm = None\n",
    "\n",
    "        B = t.size(0)\n",
    "        q = self.fuse_token.expand(B, 1, -1)\n",
    "        for blk in self.blocks:\n",
    "            attn_out, _ = blk[\"mha\"](q, kv, kv, key_padding_mask=kpm)\n",
    "            q = blk[\"ln1\"](q + attn_out)\n",
    "            q = blk[\"ln2\"](q + blk[\"ffn\"](q))\n",
    "        logits = self.head(q.squeeze(1))\n",
    "        return logits, q.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ab7724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred logits shape: torch.Size([21, 2]), fused shape: torch.Size([21, 512])\n"
     ]
    }
   ],
   "source": [
    "model = FusionTokenCrossAttn(\n",
    "    pd_d_text=text.size(-1),\n",
    "    pd_d_audio=audio.size(-1),\n",
    "    pd_d_vision=vision.size(-1),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    depth=2,\n",
    "    num_classes=2\n",
    ")\n",
    "logits, fused = model(text, audio, vision)\n",
    "print(f\"Pred logits shape: {logits.shape}, fused shape: {fused.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0874525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 512) 21 (21,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "out_path = \"/home/jovyan/Desktop/PD_LLM/codes/pd&Hc_multi/Fixed_HC_Fused_embeddings/HC_ReadText_fused_embeddings_FIXED.npz\"\n",
    "\n",
    "ids_kept = common_ids                    # the exact order you looped\n",
    "labels   = np.full((len(ids_kept),), 1, dtype=np.int64)   # PD = 1\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "# prepare arrays for saving (detach and move to cpu first)\n",
    "fused_np = fused.detach().cpu().numpy()\n",
    "ids_arr = np.array(ids_kept, dtype='<U')  # store as unicode array\n",
    "kept_idx = np.arange(len(ids_kept), dtype=np.int64)\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    fused    = fused_np,   # [B, D]\n",
    "    ids      = ids_arr,\n",
    "    labels   = labels,\n",
    "    kept_idx = kept_idx,\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "z = np.load(out_path, allow_pickle=True)\n",
    "print(z[\"fused\"].shape, len(z[\"ids\"]), z[\"labels\"].shape)\n",
    "assert z[\"fused\"].shape[0] == len(z[\"ids\"]) == z[\"labels\"].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd30efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
